{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad671be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\udits\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import tweepy\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing Scikit-learn Libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Importing Other Libraries\n",
    "import numpy as np\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For processing\n",
    "import math\n",
    "import random\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "\n",
    "# Libraries for model training\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504322c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\udits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\udits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\udits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\udits\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92134dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved to cleaned_tweets_1.csv\n",
      "                         Datetime  Tweet Id  \\\n",
      "0  Wed Jan 01 03:59:03 +0000 2014       NaN   \n",
      "1  Wed Jan 01 18:08:47 +0000 2014       NaN   \n",
      "2  Wed Jan 01 01:52:31 +0000 2014       NaN   \n",
      "3  Wed Jan 01 10:52:20 +0000 2014       NaN   \n",
      "4  Wed Jan 01 15:01:12 +0000 2014       NaN   \n",
      "\n",
      "                                                Text    Username Company  \\\n",
      "0  rt AT_USER summary of yesterday's webcast feat...  1938270918    AAPL   \n",
      "1              itv will boost apple URL $ aapl apple    23059499    AAPL   \n",
      "2  iphone users are more intelligent than samsung...    23954327    AAPL   \n",
      "3  2013 wrap-up and trading set review - part iii...    23059499    AAPL   \n",
      "4        apple screwed up big time URL $ amzn $ aapl    23669783    AAPL   \n",
      "\n",
      "         Date                                       cleaned_text  \\\n",
      "0  2014-01-01  rt atuser summary of yesterdays webcast featur...   \n",
      "1  2014-01-01               itv will boost apple url  aapl apple   \n",
      "2  2014-01-01  iphone users are more intelligent than samsung...   \n",
      "3  2014-01-01   wrapup and trading set review  part iii url  ...   \n",
      "4  2014-01-01          apple screwed up big time url  amzn  aapl   \n",
      "\n",
      "                                      processed_text  \n",
      "0  rt atuser summary yesterday webcast featuring ...  \n",
      "1                     itv boost apple url aapl apple  \n",
      "2  iphone user intelligent samsung blackberry htc...  \n",
      "3  wrapup trading set review part iii url aapl ap...  \n",
      "4               apple screwed big time url amzn aapl  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Company</th>\n",
       "      <th>average_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>BAC</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>C</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>CSCO</td>\n",
       "      <td>0.411111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22785</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22786</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>ORCL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22787</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>T</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22788</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>UNH</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22789</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>WFC</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22790 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Company  average_sentiment\n",
       "0      2014-01-01    AAPL           0.160000\n",
       "1      2014-01-01     BAC           0.225000\n",
       "2      2014-01-01       C           0.200000\n",
       "3      2014-01-01     CAT           0.000000\n",
       "4      2014-01-01    CSCO           0.411111\n",
       "...           ...     ...                ...\n",
       "22785  2015-12-31    MSFT           0.000000\n",
       "22786  2015-12-31    ORCL           0.000000\n",
       "22787  2015-12-31       T           0.066667\n",
       "22788  2015-12-31     UNH           0.000000\n",
       "22789  2015-12-31     WFC           0.000000\n",
       "\n",
       "[22790 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"stocknet_tweets_all_companies.csv\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_1 = df_1.drop_duplicates(subset=\"Text\", keep=\"first\")\n",
    "\n",
    "# Handle missing data\n",
    "df_1 = df_1.dropna(subset=[\"Text\"])\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove links, mentions, hashtags, and special characters\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Clean the text\n",
    "df_1[\"cleaned_text\"] = df_1[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df_1[\"processed_text\"] = df_1[\"cleaned_text\"].apply(preprocess_text)\n",
    "\n",
    "# Optional: Filter out non-English tweets\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# df_1 = df_1[df_1[\"processed_text\"].apply(is_english)]\n",
    "\n",
    "# Save the processed data\n",
    "df_1.to_csv(\"cleaned_tweets_1.csv\", index=False)\n",
    "print(\"Data cleaned and saved to cleaned_tweets_1.csv\")\n",
    "\n",
    "# Load the cleaned data\n",
    "df_1 = pd.read_csv(\"cleaned_tweets_1.csv\")\n",
    "print(df_1.head())\n",
    "# Function to calculate sentiment polarity\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "df_1['processed_text'] = df_1['processed_text'].fillna('')\n",
    "# Add sentiment polarity to the DataFrame\n",
    "df_1[\"sentiment\"] = df_1[\"processed_text\"].apply(get_sentiment)\n",
    "\n",
    "# Categorize sentiment\n",
    "def categorize_sentiment(polarity):\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "df_1[\"sentiment_category\"] = df_1[\"sentiment\"].apply(categorize_sentiment)\n",
    "\n",
    "# Save sentiment results\n",
    "df_1.to_csv(\"tweets_with_sentiment_1.csv\", index=False)\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "sentiment_counts = df_1[\"sentiment_category\"].value_counts()\n",
    "sentiment_counts.plot(kind=\"bar\", color=[\"green\", \"red\", \"blue\"])\n",
    "plt.title(\"Sentiment Distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.show()\n",
    "\n",
    "df_1.head(10)\n",
    "\n",
    "# Group by 'Date' and 'Company', then calculate the average sentiment\n",
    "average_sentiment_df = df_1.groupby(['Date', 'Company'])['sentiment'].mean().reset_index()\n",
    "\n",
    "# Rename the sentiment column\n",
    "average_sentiment_df.rename(columns={'sentiment': 'average_sentiment'}, inplace=True)\n",
    "average_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fad7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(stock):\n",
    "\n",
    "    # Importing Libraries\n",
    "    import tweepy\n",
    "    import csv\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from langdetect import detect\n",
    "    from textblob import TextBlob\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Importing Scikit-learn Libraries\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    from wordcloud import WordCloud\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score, StratifiedKFold\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score,\n",
    "        f1_score, classification_report, roc_curve, auc\n",
    "    )\n",
    "\n",
    "    # Importing Other Libraries\n",
    "    import numpy as np\n",
    "    import shap\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    # Basic libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # For processing\n",
    "    import math\n",
    "    import random\n",
    "    import datetime as dt\n",
    "    import matplotlib.dates as mdates\n",
    "\n",
    "    # For visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mplfinance.original_flavor import candlestick_ohlc\n",
    "\n",
    "    # Libraries for model training\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    np.random.seed(42)\n",
    "    # tf.random.set_seed(42)\n",
    "    random.seed(42)\n",
    "      # Replace 'AAPL' with the desired company symbol\n",
    "    company_name = stock\n",
    "\n",
    "    # Filter the average_sentiment_df for the specific company\n",
    "    company_df = average_sentiment_df[average_sentiment_df['Company'] == company_name].reset_index(drop=True)\n",
    "\n",
    "    # Copy the DataFrame so original isn't modified\n",
    "    company_df = company_df.copy()\n",
    "\n",
    "    # Initialize fixed_sentiment column\n",
    "    company_df['fixed_sentiment'] = 0.0\n",
    "\n",
    "    # Set the first value same as average_sentiment\n",
    "    company_df.loc[0, 'fixed_sentiment'] = company_df.loc[0, 'average_sentiment']\n",
    "    # Iterate from the second row and compute the average with the previous fixed_sentiment\n",
    "    for i in range(1, len(company_df)):\n",
    "        prev_fixed = company_df.loc[i - 1, 'fixed_sentiment']\n",
    "        current_avg = company_df.loc[i, 'average_sentiment']\n",
    "        company_df.loc[i, 'fixed_sentiment'] = (prev_fixed + current_avg) / 2\n",
    "\n",
    "    import yfinance as yf\n",
    "    import pandas as pd\n",
    "\n",
    "    # Define the stock ticker\n",
    "    ticker = stock  # You can replace with any other ticker\n",
    "\n",
    "    # Download last 200 days of data\n",
    "    # stock_data = yf.download(ticker, period='200d')\n",
    "    stock_data = yf.download(ticker, start='2014-01-01', end='2015-12-31')\n",
    "    # print(stock_data)\n",
    "    # Reset index to bring the Date into a column\n",
    "    stock_data = stock_data.reset_index()\n",
    "\n",
    "    # print(stock_data['Close'].values.flatten())\n",
    "    print(\"***************************************\")\n",
    "    print(stock_data)\n",
    "    print(\"***************************************\")\n",
    "\n",
    "    # Create the DataFrame with 'Date' and 'Close' columns\n",
    "    stock_df = pd.DataFrame({\n",
    "        'Date': stock_data['Date'].dt.date.values.flatten(),  # Ensure the Date is 1D by extracting just the date part\n",
    "        'Closing_Price': stock_data['Close'].values.flatten()  # The Close price is already 1D\n",
    "    })\n",
    "\n",
    "    last_30_days_stock = stock_df.tail(30)\n",
    "\n",
    "    # If you want the 'Closing_Price' as a list, you can extract that column:\n",
    "    last_30_closing_prices = last_30_days_stock['Closing_Price'].tolist()\n",
    "\n",
    "    full_dates = pd.date_range(start='2014-01-01', end='2015-12-31').date\n",
    "\n",
    "    # Ensure Date columns are datetime\n",
    "    company_df['Date'] = pd.to_datetime(company_df['Date'])\n",
    "    stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "\n",
    "    # Filter company_df for matching dates in stock_df\n",
    "    stock_df = stock_df[stock_df['Date'].isin(company_df['Date'])]\n",
    "\n",
    "    stock_df1 = pd.DataFrame({\n",
    "        'Closing_Price': stock_df['Closing_Price'].values.flatten()\n",
    "    })\n",
    "    print(\"***********************************************************\")\n",
    "    print(stock_df1)\n",
    "    print(\"***********************************************************\")\n",
    "    import numpy as np\n",
    "    # Normalizing our data using MinMaxScaler\n",
    "    new_df=stock_df1\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data=scaler.fit_transform(np.array(new_df).reshape(-1,1))\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    train_size = int(len(scaled_data) * 0.8)  # 80% for training\n",
    "    train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]\n",
    "\n",
    "\n",
    "    # Define the sequence length (number of past time steps)\n",
    "    n_past = 10\n",
    "\n",
    "    # Prepare sequences for LSTM\n",
    "    X_train, y_train = [], []\n",
    "    for i in range(n_past, len(train_data)):\n",
    "        X_train.append(train_data[i - n_past:i, 0])\n",
    "        y_train.append(train_data[i, 0])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "    # Similarly prepare sequences for the test set\n",
    "    X_test, y_test = [], []\n",
    "    for i in range(n_past, len(test_data)):\n",
    "        X_test.append(test_data[i - n_past:i, 0])\n",
    "        y_test.append(test_data[i, 0])\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "    # Reshape input data for LSTM([samples, time steps, features])\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    s_idx=10\n",
    "    updated_X_train = np.zeros((X_train.shape[0], X_train.shape[1] + 1, 1))\n",
    "\n",
    "    # Iterate over each row in X_train\n",
    "    for i in range(X_train.shape[0]):\n",
    "        # Copy the existing data into updated_X_train\n",
    "        updated_X_train[i, :-1, :] = X_train[i]\n",
    "\n",
    "        # Add the fixed_sentiment value from continuous_df to the last element of the row\n",
    "        updated_X_train[i, -1, 0] = company_df['fixed_sentiment'].iloc[s_idx]\n",
    "        s_idx+=1\n",
    "\n",
    "    ##model\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # First LSTM layer with 50 units, input shape, and return sequences\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(updated_X_train.shape[1], 1)))\n",
    "    model.add(Dropout(0.2))         # Adding dropout to prevent overfitting\n",
    "\n",
    "    # Second LSTM layer with 50 units and return sequences\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Third LSTM layer with 50 units\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Add a dense output layer with one unit\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "\n",
    "\n",
    "    # Defining our callbacks\n",
    "    checkpoints = ModelCheckpoint(filepath = 'my_weights.keras', save_best_only = True)\n",
    "    # Defining our early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "    # Training our lstm model\n",
    "    model.fit(updated_X_train, y_train,\n",
    "              validation_data=(X_test,y_test),\n",
    "              epochs=100,\n",
    "              batch_size=32,\n",
    "              verbose=1,\n",
    "              callbacks= [checkpoints, early_stopping])\n",
    "\n",
    "\n",
    "    # Create a new array to hold the updated X_train (150, 11, 1)\n",
    "    updated_X_test = np.zeros((X_test.shape[0], X_test.shape[1] + 1, 1))\n",
    "\n",
    "    # Iterate over each row in X_train\n",
    "    for i in range(X_test.shape[0]):\n",
    "        # Copy the existing data into updated_X_train\n",
    "        updated_X_test[i, :-1, :] = X_test[i]\n",
    "\n",
    "        # Add the fixed_sentiment value from continuous_df to the last element of the row\n",
    "        updated_X_test[i, -1, 0] = company_df['fixed_sentiment'].iloc[s_idx]\n",
    "        s_idx+=1\n",
    "\n",
    "    # Let's do the prediction and check performance metrics\n",
    "    train_predict=model.predict(updated_X_train)\n",
    "    test_predict=model.predict(updated_X_test)\n",
    "\n",
    "    import math\n",
    "    # Calculate train data RMSE\n",
    "    print(math.sqrt(mean_squared_error(y_train,train_predict)))\n",
    "    # Calculate test data RMSE\n",
    "    print(math.sqrt(mean_squared_error(y_test,test_predict)))\n",
    "    print(math.sqrt(mean_squared_error(y_train,train_predict))/np.mean(y_train))\n",
    "    print(math.sqrt(mean_squared_error(y_test,test_predict))/np.mean(y_test))\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import numpy as np\n",
    "    import math\n",
    "\n",
    "    # RMSE\n",
    "    train_rmse = math.sqrt(mean_squared_error(y_train, train_predict))\n",
    "    test_rmse = math.sqrt(mean_squared_error(y_test, test_predict))\n",
    "\n",
    "    # NRMSE\n",
    "    train_nrmse = train_rmse / np.mean(y_train)\n",
    "    test_nrmse = test_rmse / np.mean(y_test)\n",
    "\n",
    "    # MAE\n",
    "    train_mae = mean_absolute_error(y_train, train_predict)\n",
    "    test_mae = mean_absolute_error(y_test, test_predict)\n",
    "\n",
    "    # R^2 Score\n",
    "    train_r2 = r2_score(y_train, train_predict)\n",
    "    test_r2 = r2_score(y_test, test_predict)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"Train RMSE:\", train_rmse)\n",
    "    print(\"Test RMSE:\", test_rmse)\n",
    "    print(\"Train NRMSE:\", train_nrmse)\n",
    "    print(\"Test NRMSE:\", test_nrmse)\n",
    "    print(\"Train MAE:\", train_mae)\n",
    "    print(\"Test MAE:\", test_mae)\n",
    "    print(\"Train R^2 Score:\", train_r2)\n",
    "    print(\"Test R^2 Score:\", test_r2)\n",
    "\n",
    "    return model,last_30_closing_prices,scaled_data,company_df,scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949b2a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for BAC\n",
      "***************************************\n",
      "Price        Date      Close       High        Low       Open     Volume\n",
      "Ticker                   BAC        BAC        BAC        BAC        BAC\n",
      "0      2014-01-02  12.895577  12.943635  12.559171  12.567180  148709900\n",
      "1      2014-01-03  13.143873  13.215960  12.999699  13.031738  129921800\n",
      "2      2014-01-06  13.344118  13.400186  13.264021  13.320089  114431300\n",
      "3      2014-01-07  13.215961  13.448243  13.175914  13.432223  110605100\n",
      "4      2014-01-08  13.280037  13.368144  13.231979  13.352124  101036400\n",
      "..            ...        ...        ...        ...        ...        ...\n",
      "498    2015-12-23  14.160586  14.160586  13.964592  14.013590   65770700\n",
      "499    2015-12-24  14.103421  14.193251  14.062588  14.144253   29369400\n",
      "500    2015-12-28  13.989088  14.070753  13.866592  14.062586   41777500\n",
      "501    2015-12-29  14.111585  14.168749  14.013587  14.087085   45670400\n",
      "502    2015-12-30  13.923758  14.078920  13.915592  14.046255   35066400\n",
      "\n",
      "[503 rows x 6 columns]\n",
      "***************************************\n",
      "***********************************************************\n",
      "     Closing_Price\n",
      "0        12.895577\n",
      "1        13.143873\n",
      "2        13.344118\n",
      "3        13.215961\n",
      "4        13.280037\n",
      "..             ...\n",
      "414      14.160586\n",
      "415      14.103421\n",
      "416      13.989088\n",
      "417      14.111585\n",
      "418      13.923758\n",
      "\n",
      "[419 rows x 1 columns]\n",
      "***********************************************************\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.1612 - val_loss: 0.0274\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0303 - val_loss: 0.0440\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0243 - val_loss: 0.0212\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0197 - val_loss: 0.0264\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0191 - val_loss: 0.0267\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0192 - val_loss: 0.0206\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0193 - val_loss: 0.0376\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0210 - val_loss: 0.0206\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0171 - val_loss: 0.0204\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0162 - val_loss: 0.0226\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0185 - val_loss: 0.0204\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0174 - val_loss: 0.0196\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0165 - val_loss: 0.0217\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0164 - val_loss: 0.0215\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0163 - val_loss: 0.0193\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0143 - val_loss: 0.0198\n",
      "Epoch 17/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0138 - val_loss: 0.0184\n",
      "Epoch 18/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0137 - val_loss: 0.0181\n",
      "Epoch 19/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0135 - val_loss: 0.0180\n",
      "Epoch 20/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0158 - val_loss: 0.0192\n",
      "Epoch 21/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0154 - val_loss: 0.0182\n",
      "Epoch 22/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0132 - val_loss: 0.0172\n",
      "Epoch 23/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0136 - val_loss: 0.0180\n",
      "Epoch 24/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 25/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0122 - val_loss: 0.0180\n",
      "Epoch 26/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0130 - val_loss: 0.0158\n",
      "Epoch 27/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0106 - val_loss: 0.0150\n",
      "Epoch 28/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0115 - val_loss: 0.0198\n",
      "Epoch 29/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0120 - val_loss: 0.0191\n",
      "Epoch 30/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0093 - val_loss: 0.0144\n",
      "Epoch 31/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0103 - val_loss: 0.0159\n",
      "Epoch 32/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 33/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0108 - val_loss: 0.0159\n",
      "Epoch 34/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0108 - val_loss: 0.0149\n",
      "Epoch 35/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0101 - val_loss: 0.0153\n",
      "Epoch 36/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0113 - val_loss: 0.0177\n",
      "Epoch 37/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0116 - val_loss: 0.0175\n",
      "Epoch 38/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0113 - val_loss: 0.0385\n",
      "Epoch 39/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0155 - val_loss: 0.0209\n",
      "Epoch 40/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0120 - val_loss: 0.0159\n",
      "Epoch 41/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0116 - val_loss: 0.0189\n",
      "Epoch 42/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0103 - val_loss: 0.0134\n",
      "Epoch 43/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 44/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0118 - val_loss: 0.0115\n",
      "Epoch 45/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0122 - val_loss: 0.0178\n",
      "Epoch 46/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0098 - val_loss: 0.0217\n",
      "Epoch 47/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0092 - val_loss: 0.0134\n",
      "Epoch 48/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0092 - val_loss: 0.0195\n",
      "Epoch 49/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0084 - val_loss: 0.0203\n",
      "Epoch 50/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0092 - val_loss: 0.0160\n",
      "Epoch 51/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0082 - val_loss: 0.0131\n",
      "Epoch 52/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 53/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0093 - val_loss: 0.0130\n",
      "Epoch 54/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0094 - val_loss: 0.0116\n",
      "Epoch 55/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0110 - val_loss: 0.0156\n",
      "Epoch 56/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0110 - val_loss: 0.0205\n",
      "Epoch 57/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0073 - val_loss: 0.0153\n",
      "Epoch 58/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 - val_loss: 0.0197\n",
      "Epoch 59/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0077 - val_loss: 0.0157\n",
      "Epoch 60/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0085 - val_loss: 0.0176\n",
      "Epoch 61/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0082 - val_loss: 0.0147\n",
      "Epoch 62/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0077 - val_loss: 0.0109\n",
      "Epoch 63/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0115 - val_loss: 0.0176\n",
      "Epoch 64/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0084 - val_loss: 0.0144\n",
      "Epoch 65/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0086 - val_loss: 0.0221\n",
      "Epoch 66/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0072 - val_loss: 0.0138\n",
      "Epoch 67/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0079 - val_loss: 0.0237\n",
      "Epoch 68/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0076 - val_loss: 0.0141\n",
      "Epoch 69/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0081 - val_loss: 0.0187\n",
      "Epoch 70/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0081 - val_loss: 0.0175\n",
      "Epoch 71/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0082 - val_loss: 0.0200\n",
      "Epoch 72/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0080 - val_loss: 0.0151\n",
      "Epoch 73/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0094 - val_loss: 0.0151\n",
      "Epoch 74/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 - val_loss: 0.0197\n",
      "Epoch 75/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0072 - val_loss: 0.0151\n",
      "Epoch 76/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0086 - val_loss: 0.0168\n",
      "Epoch 77/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0090 - val_loss: 0.0178\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09128550249795613\n",
      "0.11551745289173868\n",
      "0.1956356470563698\n",
      "0.20762546142234492\n",
      "Train RMSE: 0.09128550249795613\n",
      "Test RMSE: 0.11551745289173868\n",
      "Train NRMSE: 0.1956356470563698\n",
      "Test NRMSE: 0.20762546142234492\n",
      "Train MAE: 0.07156875628314562\n",
      "Test MAE: 0.09222925060240325\n",
      "Train R^2 Score: 0.8155274363264089\n",
      "Test R^2 Score: 0.6618191005113863\n",
      "Training model for C\n",
      "***************************************\n",
      "Price        Date      Close       High        Low       Open    Volume\n",
      "Ticker                     C          C          C          C         C\n",
      "0      2014-01-02  39.703560  39.802307  39.354151  39.521258  16479700\n",
      "1      2014-01-03  40.561886  40.615057  39.733937  39.794702  26884900\n",
      "2      2014-01-06  40.873322  41.237922  40.584678  40.728998  28503100\n",
      "3      2014-01-07  41.154366  41.564543  40.850530  41.465796  28840400\n",
      "4      2014-01-08  41.632896  41.777217  41.169549  41.237911  25989800\n",
      "..            ...        ...        ...        ...        ...       ...\n",
      "498    2015-12-23  40.126362  40.133985  39.707029  39.874761  14950200\n",
      "499    2015-12-24  40.187351  40.385583  39.989122  40.011994   4671200\n",
      "500    2015-12-28  39.935757  40.080616  39.615537  40.080616   8761700\n",
      "501    2015-12-29  40.393196  40.576179  40.210216  40.225462  10155100\n",
      "502    2015-12-30  39.874756  40.362707  39.836635  40.286465   8763300\n",
      "\n",
      "[503 rows x 6 columns]\n",
      "***************************************\n",
      "***********************************************************\n",
      "     Closing_Price\n",
      "0        39.703560\n",
      "1        40.561886\n",
      "2        40.873322\n",
      "3        41.154366\n",
      "4        41.632896\n",
      "..             ...\n",
      "424      39.653656\n",
      "425      40.126362\n",
      "426      40.187351\n",
      "427      40.393196\n",
      "428      39.874756\n",
      "\n",
      "[429 rows x 1 columns]\n",
      "***********************************************************\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 0.1133 - val_loss: 0.0147\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0255 - val_loss: 0.0182\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0176 - val_loss: 0.0088\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0137 - val_loss: 0.0101\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0166 - val_loss: 0.0131\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0137 - val_loss: 0.0092\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0140 - val_loss: 0.0089\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0141 - val_loss: 0.0088\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0147 - val_loss: 0.0085\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0124 - val_loss: 0.0092\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0107 - val_loss: 0.0086\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0121 - val_loss: 0.0086\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0107 - val_loss: 0.0101\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0118 - val_loss: 0.0087\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0119 - val_loss: 0.0093\n",
      "Epoch 17/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0122 - val_loss: 0.0096\n",
      "Epoch 18/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0132 - val_loss: 0.0079\n",
      "Epoch 19/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0091 - val_loss: 0.0081\n",
      "Epoch 20/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0122 - val_loss: 0.0075\n",
      "Epoch 21/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0104 - val_loss: 0.0071\n",
      "Epoch 22/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0070\n",
      "Epoch 23/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0112 - val_loss: 0.0066\n",
      "Epoch 24/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0096 - val_loss: 0.0075\n",
      "Epoch 25/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0108 - val_loss: 0.0067\n",
      "Epoch 26/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0081 - val_loss: 0.0066\n",
      "Epoch 27/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0087 - val_loss: 0.0062\n",
      "Epoch 28/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0078 - val_loss: 0.0074\n",
      "Epoch 29/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0084 - val_loss: 0.0067\n",
      "Epoch 30/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0073\n",
      "Epoch 31/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0082 - val_loss: 0.0065\n",
      "Epoch 32/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0079 - val_loss: 0.0110\n",
      "Epoch 33/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0086 - val_loss: 0.0072\n",
      "Epoch 34/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0085 - val_loss: 0.0056\n",
      "Epoch 35/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0073 - val_loss: 0.0063\n",
      "Epoch 36/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0079 - val_loss: 0.0070\n",
      "Epoch 37/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0079 - val_loss: 0.0073\n",
      "Epoch 38/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0075 - val_loss: 0.0116\n",
      "Epoch 39/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0073 - val_loss: 0.0195\n",
      "Epoch 40/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 41/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0070 - val_loss: 0.0108\n",
      "Epoch 42/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 43/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0071 - val_loss: 0.0061\n",
      "Epoch 44/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0076 - val_loss: 0.0079\n",
      "Epoch 45/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0055\n",
      "Epoch 46/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0073 - val_loss: 0.0066\n",
      "Epoch 47/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 48/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0065 - val_loss: 0.0060\n",
      "Epoch 49/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 50/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0063 - val_loss: 0.0094\n",
      "Epoch 51/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0067 - val_loss: 0.0113\n",
      "Epoch 52/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 53/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 54/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0071 - val_loss: 0.0063\n",
      "Epoch 55/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0076 - val_loss: 0.0088\n",
      "Epoch 56/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0072 - val_loss: 0.0089\n",
      "Epoch 57/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069 - val_loss: 0.0090\n",
      "Epoch 58/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0066 - val_loss: 0.0106\n",
      "Epoch 59/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0067 - val_loss: 0.0080\n",
      "Epoch 60/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0058 - val_loss: 0.0072\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08250584982158977\n",
      "0.08549971279524339\n",
      "0.21417961101716737\n",
      "0.18048647830304582\n",
      "Train RMSE: 0.08250584982158977\n",
      "Test RMSE: 0.08549971279524339\n",
      "Train NRMSE: 0.21417961101716737\n",
      "Test NRMSE: 0.18048647830304582\n",
      "Train MAE: 0.06840314695364015\n",
      "Test MAE: 0.0724536612419484\n",
      "Train R^2 Score: 0.8673346677673437\n",
      "Test R^2 Score: 0.43900452979207094\n",
      "Training model for CSCO\n",
      "***************************************\n",
      "Price        Date      Close       High        Low       Open    Volume\n",
      "Ticker                  CSCO       CSCO       CSCO       CSCO      CSCO\n",
      "0      2014-01-02  15.499217  15.703526  15.435811  15.618984  44377000\n",
      "1      2014-01-03  15.485123  15.583755  15.379446  15.562619  36328200\n",
      "2      2014-01-06  15.506261  15.661253  15.449900  15.471035  34150300\n",
      "3      2014-01-07  15.717617  15.788068  15.604896  15.682392  37368800\n",
      "4      2014-01-08  15.703527  15.752843  15.604895  15.703527  38362700\n",
      "..            ...        ...        ...        ...        ...       ...\n",
      "498    2015-12-23  20.372454  20.379889  20.075046  20.104787  18803700\n",
      "499    2015-12-24  20.357580  20.483979  20.350146  20.379886   8166300\n",
      "500    2015-12-28  20.305538  20.350150  20.171704  20.298103  14204900\n",
      "501    2015-12-29  20.647558  20.684734  20.372455  20.402197  16319600\n",
      "502    2015-12-30  20.469109  20.647553  20.461673  20.565766  10620300\n",
      "\n",
      "[503 rows x 6 columns]\n",
      "***************************************\n",
      "***********************************************************\n",
      "     Closing_Price\n",
      "0        15.499217\n",
      "1        15.485123\n",
      "2        15.506261\n",
      "3        15.717617\n",
      "4        15.703527\n",
      "..             ...\n",
      "354      19.993259\n",
      "355      20.372454\n",
      "356      20.357580\n",
      "357      20.305538\n",
      "358      20.647558\n",
      "\n",
      "[359 rows x 1 columns]\n",
      "***********************************************************\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 0.2254 - val_loss: 0.0241\n",
      "Epoch 2/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0428 - val_loss: 0.0577\n",
      "Epoch 3/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0311 - val_loss: 0.0178\n",
      "Epoch 4/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0194 - val_loss: 0.0250\n",
      "Epoch 5/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0157 - val_loss: 0.0213\n",
      "Epoch 6/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0134 - val_loss: 0.0204\n",
      "Epoch 7/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0138 - val_loss: 0.0189\n",
      "Epoch 8/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0101 - val_loss: 0.0205\n",
      "Epoch 9/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0116 - val_loss: 0.0209\n",
      "Epoch 10/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0111 - val_loss: 0.0176\n",
      "Epoch 11/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0088 - val_loss: 0.0203\n",
      "Epoch 12/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0118 - val_loss: 0.0184\n",
      "Epoch 13/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0110 - val_loss: 0.0170\n",
      "Epoch 14/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0107 - val_loss: 0.0185\n",
      "Epoch 15/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0096 - val_loss: 0.0182\n",
      "Epoch 16/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0119 - val_loss: 0.0181\n",
      "Epoch 17/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0097 - val_loss: 0.0164\n",
      "Epoch 18/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0112 - val_loss: 0.0176\n",
      "Epoch 19/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0082 - val_loss: 0.0161\n",
      "Epoch 20/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0100 - val_loss: 0.0172\n",
      "Epoch 21/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0103 - val_loss: 0.0161\n",
      "Epoch 22/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0099 - val_loss: 0.0159\n",
      "Epoch 23/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0102 - val_loss: 0.0164\n",
      "Epoch 24/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080 - val_loss: 0.0151\n",
      "Epoch 25/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0100 - val_loss: 0.0150\n",
      "Epoch 26/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0096 - val_loss: 0.0159\n",
      "Epoch 27/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0099 - val_loss: 0.0146\n",
      "Epoch 28/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0106 - val_loss: 0.0156\n",
      "Epoch 29/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0090 - val_loss: 0.0150\n",
      "Epoch 30/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0086 - val_loss: 0.0149\n",
      "Epoch 31/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0095 - val_loss: 0.0163\n",
      "Epoch 32/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0094 - val_loss: 0.0172\n",
      "Epoch 33/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.0157\n",
      "Epoch 34/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0101 - val_loss: 0.0136\n",
      "Epoch 35/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0097 - val_loss: 0.0138\n",
      "Epoch 36/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0076 - val_loss: 0.0136\n",
      "Epoch 37/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0090 - val_loss: 0.0145\n",
      "Epoch 38/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0086 - val_loss: 0.0159\n",
      "Epoch 39/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0078 - val_loss: 0.0133\n",
      "Epoch 40/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0100 - val_loss: 0.0150\n",
      "Epoch 41/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0091 - val_loss: 0.0131\n",
      "Epoch 42/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0101 - val_loss: 0.0210\n",
      "Epoch 43/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0130 - val_loss: 0.0144\n",
      "Epoch 44/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0097 - val_loss: 0.0136\n",
      "Epoch 45/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0097 - val_loss: 0.0145\n",
      "Epoch 46/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0080 - val_loss: 0.0128\n",
      "Epoch 47/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0095 - val_loss: 0.0132\n",
      "Epoch 48/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0086 - val_loss: 0.0127\n",
      "Epoch 49/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0074 - val_loss: 0.0128\n",
      "Epoch 50/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0083 - val_loss: 0.0127\n",
      "Epoch 51/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0079 - val_loss: 0.0122\n",
      "Epoch 52/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0071 - val_loss: 0.0123\n",
      "Epoch 53/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0102 - val_loss: 0.0132\n",
      "Epoch 54/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0079 - val_loss: 0.0138\n",
      "Epoch 55/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0082 - val_loss: 0.0119\n",
      "Epoch 56/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0084 - val_loss: 0.0149\n",
      "Epoch 57/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0081 - val_loss: 0.0119\n",
      "Epoch 58/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0080 - val_loss: 0.0113\n",
      "Epoch 59/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0135\n",
      "Epoch 60/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0083 - val_loss: 0.0110\n",
      "Epoch 61/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0085 - val_loss: 0.0109\n",
      "Epoch 62/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0077 - val_loss: 0.0108\n",
      "Epoch 63/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0084 - val_loss: 0.0133\n",
      "Epoch 64/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0076 - val_loss: 0.0141\n",
      "Epoch 65/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0068 - val_loss: 0.0105\n",
      "Epoch 66/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0106\n",
      "Epoch 67/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0078 - val_loss: 0.0202\n",
      "Epoch 68/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0076 - val_loss: 0.0141\n",
      "Epoch 69/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0089 - val_loss: 0.0104\n",
      "Epoch 70/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0078 - val_loss: 0.0111\n",
      "Epoch 71/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0076 - val_loss: 0.0133\n",
      "Epoch 72/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0072 - val_loss: 0.0118\n",
      "Epoch 73/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0065 - val_loss: 0.0107\n",
      "Epoch 74/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0075 - val_loss: 0.0113\n",
      "Epoch 75/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0060 - val_loss: 0.0129\n",
      "Epoch 76/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0072 - val_loss: 0.0113\n",
      "Epoch 77/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0073 - val_loss: 0.0098\n",
      "Epoch 78/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0077 - val_loss: 0.0093\n",
      "Epoch 79/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0095\n",
      "Epoch 80/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0062 - val_loss: 0.0143\n",
      "Epoch 81/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0062 - val_loss: 0.0119\n",
      "Epoch 82/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0131\n",
      "Epoch 83/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0065 - val_loss: 0.0155\n",
      "Epoch 84/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0161\n",
      "Epoch 85/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0071 - val_loss: 0.0151\n",
      "Epoch 86/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0063 - val_loss: 0.0113\n",
      "Epoch 87/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0066 - val_loss: 0.0106\n",
      "Epoch 88/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0066 - val_loss: 0.0115\n",
      "Epoch 89/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0104\n",
      "Epoch 90/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0051 - val_loss: 0.0103\n",
      "Epoch 91/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0075 - val_loss: 0.0120\n",
      "Epoch 92/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.0163\n",
      "Epoch 93/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0061 - val_loss: 0.0166\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0814998355923307\n",
      "0.10080442176221652\n",
      "0.15688345527800748\n",
      "0.1369689659415834\n",
      "Train RMSE: 0.0814998355923307\n",
      "Test RMSE: 0.10080442176221652\n",
      "Train NRMSE: 0.15688345527800748\n",
      "Test NRMSE: 0.1369689659415834\n",
      "Train MAE: 0.061392432189370086\n",
      "Test MAE: 0.08153777086236227\n",
      "Train R^2 Score: 0.9255783077190044\n",
      "Test R^2 Score: 0.43030592597637063\n",
      "Training model for D\n",
      "***************************************\n",
      "Price        Date      Close       High        Low       Open   Volume\n",
      "Ticker                     D          D          D          D        D\n",
      "0      2014-01-02  39.821209  40.485103  39.733523  40.466314  2282100\n",
      "1      2014-01-03  39.777359  40.002833  39.545623  39.821203  1739100\n",
      "2      2014-01-06  39.783623  39.965255  39.614517  39.858780  1889000\n",
      "3      2014-01-07  40.516403  40.547716  39.896349  39.965244  2196600\n",
      "4      2014-01-08  40.560249  40.791983  40.259615  40.416194  2935500\n",
      "..            ...        ...        ...        ...        ...      ...\n",
      "498    2015-12-23  45.761513  45.801841  45.169998  45.169998  2334700\n",
      "499    2015-12-24  45.553135  45.734624  45.472477  45.680849   738400\n",
      "500    2015-12-28  45.613636  45.707740  45.317878  45.479204  1807900\n",
      "501    2015-12-29  46.137932  46.292534  45.674131  45.714465  1960500\n",
      "502    2015-12-30  46.164814  46.326134  45.942996  46.070710  1634900\n",
      "\n",
      "[503 rows x 6 columns]\n",
      "***************************************\n",
      "***********************************************************\n",
      "     Closing_Price\n",
      "0        39.821209\n",
      "1        39.777359\n",
      "2        39.783623\n",
      "3        40.516403\n",
      "4        40.560249\n",
      "..             ...\n",
      "400      45.761513\n",
      "401      45.553135\n",
      "402      45.613636\n",
      "403      46.137932\n",
      "404      46.164814\n",
      "\n",
      "[405 rows x 1 columns]\n",
      "***********************************************************\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - loss: 0.1611 - val_loss: 0.0256\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0351 - val_loss: 0.0301\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0220 - val_loss: 0.0087\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0139 - val_loss: 0.0147\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0121 - val_loss: 0.0089\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0121 - val_loss: 0.0106\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0093 - val_loss: 0.0094\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0104 - val_loss: 0.0087\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0087 - val_loss: 0.0102\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0109 - val_loss: 0.0084\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0094 - val_loss: 0.0085\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0102 - val_loss: 0.0116\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0120 - val_loss: 0.0099\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0113 - val_loss: 0.0094\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0090 - val_loss: 0.0084\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0088 - val_loss: 0.0088\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0100 - val_loss: 0.0087\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0086 - val_loss: 0.0082\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0075 - val_loss: 0.0093\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0079 - val_loss: 0.0083\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0110 - val_loss: 0.0098\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0091 - val_loss: 0.0088\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0104 - val_loss: 0.0077\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0085 - val_loss: 0.0077\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0090 - val_loss: 0.0075\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0100 - val_loss: 0.0083\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0087 - val_loss: 0.0073\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0091 - val_loss: 0.0073\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0084 - val_loss: 0.0077\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0077\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0112 - val_loss: 0.0073\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0080 - val_loss: 0.0071\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0089 - val_loss: 0.0077\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0086 - val_loss: 0.0067\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0077 - val_loss: 0.0065\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0102 - val_loss: 0.0068\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0090 - val_loss: 0.0065\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069 - val_loss: 0.0076\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0063 - val_loss: 0.0078\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0083 - val_loss: 0.0064\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0071 - val_loss: 0.0057\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0085 - val_loss: 0.0064\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0073 - val_loss: 0.0058\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0089 - val_loss: 0.0055\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0070 - val_loss: 0.0055\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0064 - val_loss: 0.0052\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0080 - val_loss: 0.0053\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0071 - val_loss: 0.0060\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0065 - val_loss: 0.0051\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0065 - val_loss: 0.0052\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0051 - val_loss: 0.0094\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0063 - val_loss: 0.0131\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0080 - val_loss: 0.0114\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0074 - val_loss: 0.0118\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0066 - val_loss: 0.0157\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0056 - val_loss: 0.0104\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0061 - val_loss: 0.0048\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0066 - val_loss: 0.0044\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0089 - val_loss: 0.0047\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0068 - val_loss: 0.0111\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0068 - val_loss: 0.0055\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0054 - val_loss: 0.0071\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0060 - val_loss: 0.0064\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048 - val_loss: 0.0074\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0058 - val_loss: 0.0103\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0055 - val_loss: 0.0137\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0073\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0055 - val_loss: 0.0090\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09067989242969977\n",
      "0.08344694362623697\n",
      "0.18981215173029348\n",
      "0.1595233305296817\n",
      "Train RMSE: 0.09067989242969977\n",
      "Test RMSE: 0.08344694362623697\n",
      "Train NRMSE: 0.18981215173029348\n",
      "Test NRMSE: 0.1595233305296817\n",
      "Train MAE: 0.07299653012696544\n",
      "Test MAE: 0.07087039528354547\n",
      "Train R^2 Score: 0.7356491698526719\n",
      "Test R^2 Score: 0.4869597055081297\n",
      "Training model for MSFT\n",
      "***************************************\n",
      "Price        Date      Close       High        Low       Open    Volume\n",
      "Ticker                  MSFT       MSFT       MSFT       MSFT      MSFT\n",
      "0      2014-01-02  30.996407  31.196600  30.946357  31.154891  30632200\n",
      "1      2014-01-03  30.787886  31.046468  30.529303  31.029785  31134800\n",
      "2      2014-01-06  30.137270  30.771211  30.120587  30.737844  43603700\n",
      "3      2014-01-07  30.370817  30.437549  30.203990  30.304088  35802800\n",
      "4      2014-01-08  29.828636  30.145608  29.678495  30.028830  59971700\n",
      "..            ...        ...        ...        ...        ...       ...\n",
      "498    2015-12-23  49.138596  49.191415  48.804079  49.032960  27279800\n",
      "499    2015-12-24  49.006546  49.261835  48.795275  49.173806   9558500\n",
      "500    2015-12-28  49.253048  49.253048  48.399151  48.724863  22458300\n",
      "501    2015-12-29  49.781223  50.045314  49.349876  49.552346  27731400\n",
      "502    2015-12-30  49.569969  49.983710  49.552363  49.710818  21704500\n",
      "\n",
      "[503 rows x 6 columns]\n",
      "***************************************\n",
      "***********************************************************\n",
      "     Closing_Price\n",
      "0        30.996407\n",
      "1        30.787886\n",
      "2        30.137270\n",
      "3        30.370817\n",
      "4        29.828636\n",
      "..             ...\n",
      "426      49.032963\n",
      "427      48.267113\n",
      "428      48.724854\n",
      "429      49.138596\n",
      "430      49.569969\n",
      "\n",
      "[431 rows x 1 columns]\n",
      "***********************************************************\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 0.1006 - val_loss: 0.0149\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0131 - val_loss: 0.0835\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0102 - val_loss: 0.0312\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0068 - val_loss: 0.0393\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0045 - val_loss: 0.0266\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0053 - val_loss: 0.0167\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0061 - val_loss: 0.0207\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0056 - val_loss: 0.0239\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0046 - val_loss: 0.0185\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0052 - val_loss: 0.0218\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0045 - val_loss: 0.0269\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0051 - val_loss: 0.0207\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0052 - val_loss: 0.0213\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0044 - val_loss: 0.0211\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0042 - val_loss: 0.0196\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0043 - val_loss: 0.0170\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "0.13736859100596335\n",
      "0.10457151280687658\n",
      "0.3646339171568193\n",
      "0.15022281230595896\n",
      "Train RMSE: 0.13736859100596335\n",
      "Test RMSE: 0.10457151280687658\n",
      "Train NRMSE: 0.3646339171568193\n",
      "Test NRMSE: 0.15022281230595896\n",
      "Train MAE: 0.12623905109362277\n",
      "Test MAE: 0.07624699986574575\n",
      "Train R^2 Score: 0.267502586117489\n",
      "Test R^2 Score: 0.7781980956096821\n"
     ]
    }
   ],
   "source": [
    "models = {}  # Dictionary to hold the models\\\n",
    "hist_30 = {}\n",
    "scaled = {}\n",
    "company_df = {}\n",
    "scalers = {}\n",
    "\n",
    "for company in [\"BAC\",\"C\",\"CSCO\",\"D\",\"MSFT\"]:\n",
    "    print(f\"Training model for {company}\")\n",
    "    model,hist,scaled_data,company_d,scaler = train_model(company)  # Train the model for each company\n",
    "\n",
    "    # Store the trained model in the dictionary with the company name as the key\n",
    "    models[company] = model\n",
    "    hist_30[company] = hist\n",
    "    scaled[company] = scaled_data\n",
    "    company_df[company] = company_d\n",
    "    scalers[company] = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5148f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(company):\n",
    "  ndp = scaled[company][-10:]\n",
    "  last_sentiment = company_df[company]['fixed_sentiment'].iloc[-1]\n",
    "  ndp = np.vstack([ndp, [[last_sentiment/2]]])\n",
    "  ndp = ndp.reshape(1, 11, 1)\n",
    "  next_day_predict=models[company].predict(ndp)\n",
    "  n_predict=scalers[company].inverse_transform(next_day_predict)\n",
    "  hist_30[company].append(n_predict)\n",
    "  return hist_30[company]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab5a426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hist_30.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(models, 'models.pkl') \n",
    "joblib.dump(scaled, 'scaled.pkl') \n",
    "joblib.dump(company_df, 'company_df.pkl') \n",
    "joblib.dump(scalers, 'scalers.pkl') \n",
    "joblib.dump(hist_30, 'hist_30.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d02041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.185980796813965,\n",
       " 14.527999877929688,\n",
       " 14.40584945678711,\n",
       " 14.37328052520752,\n",
       " 14.22669506072998,\n",
       " 14.22669506072998,\n",
       " 14.20226764678955,\n",
       " 14.234838485717773,\n",
       " 14.194119453430176,\n",
       " 14.503572463989258,\n",
       " 14.389245986938477,\n",
       " 14.127918243408203,\n",
       " 14.53624153137207,\n",
       " 14.323917388916016,\n",
       " 14.038091659545898,\n",
       " 13.96458911895752,\n",
       " 14.04625415802002,\n",
       " 13.662435531616211,\n",
       " 13.719594955444336,\n",
       " 14.225916862487793,\n",
       " 14.49540901184082,\n",
       " 14.127918243408203,\n",
       " 13.686933517456055,\n",
       " 13.85842514038086,\n",
       " 13.948254585266113,\n",
       " 14.1605863571167,\n",
       " 14.103421211242676,\n",
       " 13.98908805847168,\n",
       " 14.111584663391113,\n",
       " 13.923757553100586,\n",
       " array([[13.770441]], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_model(\"BAC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd26fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
